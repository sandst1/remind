# RealMem Environment Variables
# Copy this file to .env and fill in your values

# =============================================================================
# LLM PROVIDERS (choose one or more based on your setup)
# =============================================================================

# Anthropic Claude API (for consolidation and reflection)
# Required if using: --llm anthropic (default)
# Get your key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-...

# OpenAI API (for LLM and/or embeddings)
# Required if using: --llm openai OR --embedding openai (default for embeddings)
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-...

# =============================================================================
# LOCAL MODELS (Ollama - no API key needed)
# =============================================================================

# Ollama runs locally, no API key required
# Install from: https://ollama.ai/
# Then pull models:
#   ollama pull llama3.2          # For LLM
#   ollama pull nomic-embed-text  # For embeddings
#
# Use with: --llm ollama --embedding ollama

# Optional: Custom Ollama URL (default: http://localhost:11434)
# OLLAMA_BASE_URL=http://localhost:11434

# =============================================================================
# CONFIGURATION COMBINATIONS
# =============================================================================

# Cloud-only (recommended for quality):
#   --llm anthropic --embedding openai
#   Requires: ANTHROPIC_API_KEY, OPENAI_API_KEY
#
# Fully local (private, no API costs):
#   --llm ollama --embedding ollama
#   Requires: Ollama running with llama3.2 and nomic-embed-text models
#
# Hybrid (local LLM, cloud embeddings):
#   --llm ollama --embedding openai
#   Requires: OPENAI_API_KEY, Ollama running
#
# OpenAI-only:
#   --llm openai --embedding openai
#   Requires: OPENAI_API_KEY
